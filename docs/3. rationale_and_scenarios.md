### Process periodicity   

The ETL process would benefit from running from an Airflow machine, with a __monthly periodicity__, given that this is the rate at which our main data source, __flights__, is updated. Also this work is intended to study the effects of COVID in macro indicators, not to run a day-to-day operation. For these strategic purposes, monthly periodicity would suffice.  
However, given the pressing needs on this subject on many aspects of our society, we could potentially evolve the ETL to run on a weekly basis. That would require some adaptations, such as a different source for flights data that updated more often, and the commitmen of more resources (ETL machines).  
For lower cardinality tables, we could also increase to daily runs, since this is low to medium cardinalit available, that requires few resources. We could in this case, benefit from creating a separate DAG for both these processes.   

For our scenario though, monthly is a good choice.   

### Tools   
__Python / Airflow__- for process orchestration, DAG implementation and assuring periodicity of runs 
__Python / Pandas__- mainly to deal low to medium cardinality data sources (tens of thousands of rows), that can be made locally and in-memory  
__AWS EMR__- for high cardinality processes (big data) and big data sets, such as flights. Leverage on-demand, elastic processing power of variable number of worker nodes 
__AWS Redshift__ - provides powerful columnar, analytical DB through familiar SQL interface, serving analytical purpose of this project, for high cardinality tables   

### Other scenarios: Answer to udacity scenarios  

- What if the data increases by 100x ?  
Data volume is a problem in itself at this stage. Months with between 2-4 million rows of data are not uncommon. While such an increase in data would be highly unlikely (if not impossible), if it did increase we would have to:  
(1) try to process smaller batches of data at a time, by increasing the periodicity of processes ran, probably several times a day;
(2) optimize our resources, identifying clearly the needs in terms of processing resources and separating high cardinality tables from low to medium cardinality  
(3) we would have to increase the processing capacity within our pipeline. 100x increase is highly significant, so we would have to scale accordingly our operation and processing power  
(4) re-define our data model and our facts granularity (do we really need to store each flight information?)  
 
- What if the pipelines were ran daily at 7am ?  
That wouldn't pose a problem for most of our process, but we would benefit for business information, in order to optimize our pipeline. Only certain sources are updated with such periodicity, so running the whole process each day wouldn't benefit us in any way, while consuming a lot of resources. We should optimize for daily deltas and process only tables and rows that are subject to daily change.  
Data analysis of this kind would also benefit very little from such high periodicity.  

- What if the database was needed by 100+ people at the same time ?  
We would need to migrate this data from an analytical database to an operational one, with high availability. The best candidates for this, as we know, are NoSQL databases, such as Cassandra, that can be consulted by many users at the same time, while retaining great performance and reliability levels.  
Columnar databases such as Redshift are not suitable for this use case, as they are intended for analytic purposes, and relational DBs such as MySQL would probably struggle, be sluggish or break with high number of concurrent requests.   