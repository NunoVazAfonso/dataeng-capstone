{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Upload raw data files into S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Imports and Configs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format, to_timestamp, to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('configs/global.cfg')\n",
    "\n",
    "KEY = config.get('AWS', 'AWS_ACCESS_KEY_ID')\n",
    "SECRET = config.get('AWS','AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "input_path = config.get('PATH', 'INPUT_DATA_FOLDER')\n",
    "output_path = config.get('PATH', 'OUTPUT_DATA_FOLDER')\n",
    "\n",
    "raw_flight_data_path = input_path + config.get('PATH', 'FLIGHTS_RAW_FOLDER')\n",
    "raw_tweets_data_path = input_path + config.get('PATH', 'TWEETS_RAW_FOLDER')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    - Create or retrieve existing spark session\n",
    "    \n",
    "    Returns: \n",
    "        spark -- SparkSession object \n",
    "    \"\"\"\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark_session()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Load flights df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_df = spark.read.options( \n",
    "            recursiveFileLookup=True , \n",
    "            inferSchema=True, \n",
    "            header=True)\\\n",
    "        .csv( raw_flight_data_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct, desc\n",
    "\n",
    "flights_df.select(\"callsign\").groupBy(\"callsign\")\\\n",
    "    .agg( countDistinct(\"callsign\").alias(\"count\") )\\\n",
    "    .sort( desc(\"count\") )\\\n",
    "    .limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flights_df.limit(20).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_staging = flights_df.selectExpr( \"callsign\", \"icao24 as trasponder_id\", \n",
    "                      \"registration as aircraft_id\", \"typecode as aircraft_type\",\n",
    "                     \"origin as depart_airport_id\", \"destination as arrival_airport_id\",\n",
    "                        \"firstseen as depart_at\", \"lastseen as arrival_at\")\\\n",
    "    .filter(\"arrival_airport_id is not null\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cardinality of sample test 2021-03-12: 6.109.738"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3.1 Enrich airport info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "spark.sparkContext.addFile(\"https://ourairports.com/data/airports.csv\")\n",
    "\n",
    "airports_df = spark.read.csv(\"file://\" +SparkFiles.get(\"airports.csv\"), header=True, inferSchema= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63078"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airports_staging = airports_df.selectExpr(\"id\", \"ident as code\", \"type\", \"name\", \"iso_country\", \"municipality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cardinality full dataset test 2021-03-10: 63.078 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4 Load Tweets df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = spark.read.options( \n",
    "            recursiveFileLookup=True , \n",
    "            inferSchema=True, \n",
    "            header=True)\\\n",
    "        .json( raw_tweets_data_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2302853"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_staging = tweets_df.select(['date', 'keywords', 'location.country', 'tweet_id'])\\\n",
    "    .filter( col(\"location\").isNotNull() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cardinality of sample test 2021-03-12 : 2.302.853 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Load to output (S3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_staging.write.parquet(output_path + \"tweets.parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_staging.write.parquet(output_path + \"flights.parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_staging.write.parquet(output_path + \"airports.parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6109738"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet( output_path + \"flights.parquet\" ).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 Stop spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
